---
title:  6.6. Definition of Regularization
tags: 강의 인공지능_및_기계학습
mathjax: true---


Files: https://strutive07.github.io/assets/images/6_6_Definition_of_Regularization/IE661-Week_6-Part_3-icmoon-ver-1.pdf
last update datetime: Mar 07, 2020 8:39 PM

![https://strutive07.github.io/assets/images/6_6_Definition_of_Regularization/Untitled.png](https://strutive07.github.io/assets/images/6_6_Definition_of_Regularization/Untitled.png)

regularization은 perfect fit을 포기하는것이다.

perfect fit을 희생하면, test 시의 accuracy를 올리기 위함이다. 결국 미래의 데이터에 의한 variacne error를 줄이기 위함이다.

objective function에 하나의 목적을 더 추가하는것 '가지고 있는 데이터에 너무 overfit 되지 말자' 라는 목적.

 

![https://strutive07.github.io/assets/images/6_6_Definition_of_Regularization/Untitled%201.png](https://strutive07.github.io/assets/images/6_6_Definition_of_Regularization/Untitled%201.png)

![https://strutive07.github.io/assets/images/6_6_Definition_of_Regularization/Untitled%202.png](https://strutive07.github.io/assets/images/6_6_Definition_of_Regularization/Untitled%202.png)

위 objective funciton 들 처럼, objective function에 regularization term 을 추가하는 형태가 된다.

regularization 의 종류는 꽤 많고, 보통 norm 형태로 나타낸다.

L1 norm 을 사용하는 L1 regularizaiton은 lasso regularization 이라고도 부른다.

L1 regularization의 경우, 무조건 weight 에서 특정 상수를 빼는 형식으로 학습하게된다. 따라서 가중치가 적은 parameter들은 0에 수렴하게된다. 이를 통해 weight 가 높은 parameter 만 쓰고 싶다면 l1 regularization 을 통해 feature selection을 할 수 있다.

L2 regularization 은 weight decay 형식으로 weight 가 비정상 적으로 커지는것을 막을 수 있다.

[[Part Ⅲ. Neural Networks 최적화] 2. Regularization - 라온피플 머신러닝 아카데미 -](https://m.blog.naver.com/laonple/220527647084)

[딥러닝 용어 정리, L1 Regularization, L2 Regularization 의 이해, 용도와 차이 설명](https://light-tree.tistory.com/125)