---
title:  2.5 How to create a decision tree given a training dataset
tags: 강의 인공지능_및_기계학습
mathjax: true
---


Files: https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/IE661-Week_2-Part_3-icmoon-ver-1.pdf
last update datetime: Jan 04, 2020 5:58 PM

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled.png)

13개의 independent한 value와 1개의 dependent한 value로 이루어진 dataset이 있다고 가정해보자.

우리가 PAC learning(Probably approximately correct learning)

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%201.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%201.png)

우리의 hypothesis는  house value는 각 feature들을 learnaly weighted sum 한 결과 라고 가정해보자.

theata zero라는 절편을 수식으로 넣어 matrix꼴로 일반화를 하면 다음과 같이 변한다. 절편에 대한 weight를 1이라고 가정하면 합칠 수 있다.

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%202.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%202.png)

위 hypotheses에 hat이 붙는 이유는, real world에서는 항상 noise가 붙는다. 우리가 가정한 hypotheses에는 noise가 없으므로 가상의 함수이니 hat을 붙여둔것이다.

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%203.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%203.png)

X는 이미 정해져있는것이므로, error를 최소화 하는 theata를 찾으면 우리의 가정이 real world function과 유사해질것이다.

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%204.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%204.png)

제곱을 준 이유는 다음 강의에서 나온다한다.

matrix 형태이고, 상수 term을 제거하면서 수식을 정리한다.

이번에도 극점을 찾기 위해 theta로 해당 식을 미분해보자.

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%205.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%205.png)

feature가 1개라고 가정하고 그래프를 그려보면 다음과같다.

feauture가 1개일때 절편 theta zero, feature theta one 이 존재한다 따라서 아래와같은 일차 함수가 나오게된다.

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%206.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/Untitled%206.png)

만약에 error가 normal distribution을 따른다면, MLE를 한것과 동일하다.

왜그럴까? 지금 우리가 한것은 통칭 MSE, Mean squared error 인데?

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/8694C2F2-E593-4F9E-A67C-C870F8C3C356.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/8694C2F2-E593-4F9E-A67C-C870F8C3C356.png)

![https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/79C89037-5056-499F-A74B-BBCC7BEAB11E.png](https://strutive07.github.io/assets/images/2_5_How_to_create_a_decision_tree_given_a_training/79C89037-5056-499F-A74B-BBCC7BEAB11E.png)

참고) [https://ratsgo.github.io/statistics/2017/09/23/MLE/](https://ratsgo.github.io/statistics/2017/09/23/MLE/)

[https://ko.wikipedia.org/wiki/최대가능도_방법](https://ko.wikipedia.org/wiki/%EC%B5%9C%EB%8C%80%EA%B0%80%EB%8A%A5%EB%8F%84_%EB%B0%A9%EB%B2%95)