---
title:  5.6. Rethinking of SVM
tags: 강의 인공지능_및_기계학습
mathjax: true---


Files: https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/IE661-Week_5-Part_3-icmoon-ver-1.pdf
last update datetime: Mar 01, 2020 8:09 PM

이번 강의에서는 non-linear 한 SVM 을 만들기 위해 kernel trick을 배워보자.

![https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled.png](https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled.png)

기존의 soft margin SVM 을 생각해보자.

![https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%201.png](https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%201.png)

이런경우 decision boundary가 linear 할 경우 좋은 구분을 하기 힘들다.

![https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%202.png](https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%202.png)

None linear 하게 생각하면 이 데이터를 분류할 수 있을것이다.

어떻게? 

일단 간단히 생각해보자.

데이터의 차원을 늘리면 우리는 non-linear한 decision boundary를 만들 수 있을것이다. logistic regression에서 경험했듯이.

![https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%203.png](https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%203.png)

그럼 위에 그래프가 나올것이다. (결과를 다시 2차원으로 줄여서 본다면)

여기서 quadratic programming을 잘 알아야 내용을 이해할 수 있다.

![https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%204.png](https://strutive07.github.io/assets/images/5_6_Rethinking_of_SVM/Untitled%204.png)

SVM 은 Convex optimization 문제중 하나인, quadratic programming 으로 optimization 이 된다.

kernel SVM 에서는 duality 를 사용하는듯 하다. duality 를 사용할려면, Convex optimization 에서 배웠던 lagrange method를 사용해야한다.

관련 내용