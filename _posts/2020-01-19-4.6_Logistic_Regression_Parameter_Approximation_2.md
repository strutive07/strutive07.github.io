
--- 
title:  4.6. Logistic Regression Parameter Approximation 2 
tags: 강의 인공지능_및_기계학습
mathjax: true
---



Files: https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/IE661-Week_4-Part_2-icmoon-ver-1.pdf
last update datetime: Jan 19, 2020 11:30 AM

4.4, 4.5에서 배웠던 gradient descent 를 logistic regression에 적용해보자.

## Finding θ with Gradient Ascent

![https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled.png](https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled.png)

gradient ascent는 다음과같이 진행할 수있다(4.4, 4.5 참고)

$$x_{t+1} = x_t + hu' = x_t + h \cfrac{f'(x)}{|f'(x)|}$$

ascent이므로 unit vector의 방향이 미분값과 동일한 방향을 가야하므로 + 로 둔다.

(각도는 cosine 에서 구할 수있다. cosine에서 가장 큰 값을 가져올려면 각도가 0 또는 2파이 이여야 하므로 같은 방향이 된다.)

![https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled%201.png](https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled%201.png)

첫 값은 임의로 정하면 된다. 이제 이 식을 계속 반복하면서 global minimum을 찾아가면 된다.

## Gradient descent/ascent in linear regression

linear regression에서는 theta를 close form solution으로 바로 구할 수 있었다.

![https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled%202.png](https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled%202.png)

하지만 X, Y 가 매우 큰 matrix가 된다면, transpose 나 inverse 같은 연산이 매우 무거워진다. 이때도 open form solution인 gradient method를 사용하여 global minimum을 근사해여 찾을 수 있다.

![https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled%203.png](https://strutive07.github.io/assets/images/4_6_Logistic_Regression_Parameter_Approximation_2/Untitled%203.png)