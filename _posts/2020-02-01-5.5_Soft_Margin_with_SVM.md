---
title:  5.5. Soft Margin with SVM
tags: 강의 인공지능_및_기계학습
mathjax: true
---


Files: https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/IE661-Week_5-Part_2-icmoon-ver-1.pdf
last update datetime: Feb 01, 2020 11:27 PM

![https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled.png](https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled.png)

soft margin은 특정 node들이 decision boundary를 벗어나도 괜찮다! 이에 패널티를 부여할것이다, 최댛나 패널티를 줄이는 방향으로 decision boundary를 설정해라! 라는 방식이다.

![https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%201.png](https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%201.png)

패널티를 얼마나 줄것인지에 대한 상수 C 를 어떻게 정하느냐에 따라 decision boundary의 위치가 상당히 달라질 수 있다.

## Comparison to Logistic Regression

### Loss function

![https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%202.png](https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%202.png)

SVM 에서는 패널티를 부여하기 위해 hinge loss 라는것을 정의했다.

이를 더 일반화 해보면, Loss function이란것을 생각할 수 있을것이다.

하지만 우리는 logistic regression에서도 loss function을 사용했었다.

이때는 log loss를 사용했었다.

![https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%203.png](https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%203.png)

logistic regression의 식을 전개하면 위와같은 꼴이 됬었는데, 뒤에 붙은 log term이 SVM의 slack variable과 유사하게 동작한다.

![https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%204.png](https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%204.png)

![https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%205.png](https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%205.png)

검은색이 zero-one loss, 파랑색이 log loss, 빨강색이 hinge loss를 나타낸것이다.

## Strength of the loss function

![https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%206.png](https://strutive07.github.io/assets/images/5_5_Soft_Margin_with_SVM/Untitled%206.png)

그럼 loss 얼마나 많이 줄것인지에 대한 변수인 C 는 어떻게 설정해야할까?

C 를 약하게 설정하면 loss를 주지 않는것과 비슷하니, 어느정도 강하게 주어야한다.

하지만, 각 식에서 c 의 값이 특정 값을 넘어갈경우, 아무리 많이 키워도 비슷한 결과를 도출해낼것이다.

왜냐하면, C 값이 Weight term 보다 월등히 클 경우, W 값을 최소화 하는 과정에서 비슷한 결과를 가져올것이다. 마치 big-o notation에서 상수값이 의미를 잃는것과 비슷한것같다. 따라서 C 값을 가능한 크게 설정하라는 주장도 있다고 한다.

하지만, 데이터의 특성에 따라서 어떻게 달라질지는 모르는일이기때문에, c값을 바꿔가면서 실험을 할 필요는 있다고 한다.